#!/bin/ksh --login
#------------------------------------------------------------------------------#
# batch directives
#------------------------------------------------------------------------------#
#PBS -S /bin/ksh
#PBS -A PROJECT_ACCOUNT
#PBS -N asyncIoTestProto
#PBS -o asyncIoTestProto.log
#PBS -j oe
#PBS -m abe
#PBS -l walltime=00:30:00
#PBS -l select=2:ncpus=32
#PBS -l place=scatter:excl
#PBS -q standard
#PBS -W umask=0027

cd $PBS_O_WORKDIR

# Load modules and setup runtime environment
# module load intel hdf5 netcdf
# module list

export NP=$PBS_NP
export RUNDIR=`pwd`

# Enable FORTRAN RTL core dumps for severe run time failures
#export decfort_dump_flag=y

# Enable runtime compliance checker
#export ESMF_RUNTIME_COMPLIANCECHECK=ON:depth=4

# Enable runtime trace output
export ESMF_RUNTIME_TRACE=ON
#export ESMF_RUNTIME_TRACE_PETLIST="0"

# Set resource usage limits
ulimit -c unlimited #coredumpsize
ulimit -s unlimited #stacksize
ulimit -t unlimited #cputime
ulimit -d unlimited #datasize
ulimit -m unlimited #memoryuse

# Run runconfig.esmf_async using MPI
mkdir -p $RUNDIR/esmf_async
cd $RUNDIR/esmf_async
echo "Model started esmf_async:  " `date`
start=$SECONDS
mpirun -n 64 ../offlineApp.exe ../runconfig.esmf_async > run.log 2>&1
elapsed=$SECONDS-start
printf 'Model ended esmf_async: Elapsed %d:%02d:%02d\n' \
  $((elapsed / 3600)) $((elapsed / 60 % 60)) $((elapsed % 60))

# Run runconfig.esmf_fbwrite using MPI
mkdir -p $RUNDIR/esmf_fbwrite
cd $RUNDIR/esmf_fbwrite
echo "Model started esmf_fbwrite:  " `date`
start=$SECONDS
mpirun -n 64 ../offlineApp.exe ../runconfig.esmf_fbwrite > run.log 2>&1
elapsed=$SECONDS-start
printf 'Model ended esmf_fbwrite: Elapsed %d:%02d:%02d\n' \
  $((elapsed / 3600)) $((elapsed / 60 % 60)) $((elapsed % 60))

# Run runconfig.gather using MPI
mkdir -p $RUNDIR/gather
cd $RUNDIR/gather
echo "Model started gather:  " `date`
start=$SECONDS
mpirun -n 64 ../offlineApp.exe ../runconfig.gather > run.log 2>&1
elapsed=$SECONDS-start
printf 'Model ended gather: Elapsed %d:%02d:%02d\n' \
  $((elapsed / 3600)) $((elapsed / 60 % 60)) $((elapsed % 60))

# Run runconfig.nc_parHDF5_collective using MPI
mkdir -p $RUNDIR/nc_parHDF5_collective
cd $RUNDIR/nc_parHDF5_collective
echo "Model started nc_parHDF5_collective:  " `date`
start=$SECONDS
mpirun -n 64 ../offlineApp.exe ../runconfig.nc_parHDF5_collective > run.log 2>&1
elapsed=$SECONDS-start
printf 'Model ended nc_parHDF5_collective: Elapsed %d:%02d:%02d\n' \
  $((elapsed / 3600)) $((elapsed / 60 % 60)) $((elapsed % 60))

# DISABLED - EXTREMELY SLOW
# Run runconfig.nc_parHDF5_independent using MPI
# mkdir -p $RUNDIR/nc_parHDF5_independent
# cd $RUNDIR/nc_parHDF5_independent
# echo "Model started nc_parHDF5_independent:  " `date`
# start=$SECONDS
# mpirun -n 64 ../offlineApp.exe ../runconfig.nc_parHDF5_independent > run.log 2>&1
# elapsed=$SECONDS-start
# printf 'Model ended nc_parHDF5_independent: Elapsed %d:%02d:%02d\n' \
#   $((elapsed / 3600)) $((elapsed / 60 % 60)) $((elapsed % 60))

# Run runconfig.nc_pnetcdf_collective using MPI
mkdir -p $RUNDIR/nc_pnetcdf_collective
cd $RUNDIR/nc_pnetcdf_collective
echo "Model started nc_pnetcdf_collective:  " `date`
start=$SECONDS
aprun -n 64 ../offlineApp.exe ../runconfig.nc_pnetcdf_collective > run.log 2>&1
elapsed=$SECONDS-start
printf 'Model ended nc_pnetcdf_collective: Elapsed %d:%02d:%02d\n' \
  $((elapsed / 3600)) $((elapsed / 60 % 60)) $((elapsed % 60))

# Run runconfig.nc_pnetcdf_independent using MPI
mkdir -p $RUNDIR/nc_pnetcdf_independent
cd $RUNDIR/nc_pnetcdf_independent
echo "Model started nc_pnetcdf_independent:  " `date`
start=$SECONDS
aprun -n 64 ../offlineApp.exe ../runconfig.nc_pnetcdf_independent > run.log 2>&1
elapsed=$SECONDS-start
printf 'Model ended nc_pnetcdf_independent: Elapsed %d:%02d:%02d\n' \
  $((elapsed / 3600)) $((elapsed / 60 % 60)) $((elapsed % 60))

exit
